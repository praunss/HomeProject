{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone - Financial Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import fundamental packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "#import pandas_datareader.data as web\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import seaborn as sn\n",
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from matplotlib import pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from matplotlib.pyplot import savefig\n",
    "import time\n",
    "from scipy.misc import imresize\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, LSTM, SimpleRNN\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get financial data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this section if you want to download the data yourself. It is not compulsary, as the data is stored as \"Data-5year-2012-2017.csv\", and loaded in the relevant sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Financial data is downloaded via the quandl API (https://www.quandl.com/)\n",
    "    - Time interval: 2012-01-01 -- 2017-01-01\n",
    "    - Data type: Adj. Close from NASDAQ 100 (Here: Only 88 companies, 12 had too many missing values)\n",
    "- .csv file is saved on HDD for later processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import quandl\n",
    "quandl.ApiConfig.api_key = \"U5cJSsnv4Ad7UUnHNGu8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nasdaq 100 Company list\n",
    "companies = [\"WIKI/ATVI.11\",\"WIKI/ADBE.11\",\"WIKI/AKAM.11\",\"WIKI/ALXN.11\",\"WIKI/GOOGL.11\",\"WIKI/AMZN.11\",\"WIKI/AAL.11\",\"WIKI/AMGN.11\",\"WIKI/ADI.11\",\"WIKI/AAPL.11\",\"WIKI/AMAT.11\",\"WIKI/ADSK.11\",\"WIKI/ADP.11\",\"WIKI/BIDU.11\",\"WIKI/BIIB.11\",\"WIKI/BMRN.11\",\"WIKI/CA.11\",\"WIKI/CELG.11\",\"WIKI/CERN.11\",\"WIKI/CHKP.11\",\"WIKI/CTAS.11\",\"WIKI/CSCO.11\",\"WIKI/CTXS.11\",\"WIKI/CTSH.11\",\"WIKI/CMCSA.11\",\"WIKI/COST.11\",\"WIKI/CSX.11\",\"WIKI/XRAY.11\",\"WIKI/DISCA.11\",\"WIKI/DISH.11\",\"WIKI/DLTR.11\",\"WIKI/EBAY.11\",\"WIKI/EA.11\",\"WIKI/EXPE.11\",\"WIKI/ESRX.11\",\"WIKI/FAST.11\",\"WIKI/FISV.11\",\"WIKI/GILD.11\",\"WIKI/HAS.11\",\"WIKI/HSIC.11\",\"WIKI/HOLX.11\",\"WIKI/IDXX.11\",\"WIKI/ILMN.11\",\"WIKI/INCY.11\",\"WIKI/INTC.11\",\"WIKI/INTU.11\",\"WIKI/ISRG.11\",\"WIKI/JBHT.11\",\"WIKI/KLAC.11\",\"WIKI/LRCX.11\",\"WIKI/LBTYA.11\",\"WIKI/MAR.11\",\"WIKI/MAT.11\",\"WIKI/MXIM.11\",\"WIKI/MCHP.11\",\"WIKI/MU.11\",\"WIKI/MDLZ.11\",\"WIKI/MSFT.11\",\"WIKI/MNST.11\",\"WIKI/MYL.11\",\"WIKI/NFLX.11\",\"WIKI/NVDA.11\",\"WIKI/ORLY.11\",\"WIKI/PCAR.11\",\"WIKI/PAYX.11\",\"WIKI/PCLN.11\",\"WIKI/QCOM.11\",\"WIKI/REGN.11\",\"WIKI/ROST.11\",\"WIKI/STX.11\",\"WIKI/SIRI.11\",\"WIKI/SWKS.11\",\"WIKI/SBUX.11\",\"WIKI/SYMC.11\",\"WIKI/TSLA.11\",\"WIKI/TXN.11\",\"WIKI/TSCO.11\",\"WIKI/TMUS.11\",\"WIKI/FOX.11\",\"WIKI/ULTA.11\",\"WIKI/VRSK.11\",\"WIKI/VRTX.11\",\"WIKI/VIAB.11\",\"WIKI/VOD.11\",\"WIKI/WBA.11\",\"WIKI/WDC.11\",\"WIKI/WYNN.11\",\"WIKI/XLNX.11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.14187097549438\n"
     ]
    }
   ],
   "source": [
    "#Download via API\n",
    "tickerstart = time.time()\n",
    "mydata = quandl.get(companies, start_date=\"2012-01-01\", end_date=\"2017-01-01\")\n",
    "tickerend = time.time()\n",
    "print(tickerend-tickerstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save as csv\n",
    "mydata.to_csv(\"Data-5year-2012-2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensor Generation for Neural Networks, from complete dataframe that has been generated with quandl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the 5 year dataframe that has been downloaded via quandl will be transformed into input images and targets. This section has to be run in order to supply the ANNs with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the tensor is created\n",
    "- Normalize entire 5 year dataframe for each company seperately\n",
    "- Extract last 250 available data points (approx. 1 year of data) => 1 image \n",
    "- Shift one day for 5 years (2012-2017) -> ~250 images per year, Approximation:(5 * 250)-30 = 1220 images\n",
    "- Final output: np array with dimensions: 1220 x 88 x 250 (1220 images x 88 companies x 250 timepoints in image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and PCA for sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata=pd.read_csv(\"Data-5year-2012-2017.csv\")#,index_col=\"Date\")\n",
    "mydata[['Date']] = mydata[['Date']].apply(pd.to_datetime, errors='ignore')\n",
    "mydata=mydata.set_index(mydata[\"Date\"])\n",
    "mydata = mydata.drop(\"Date\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WIKI/ATVI - Adj. Close</th>\n",
       "      <th>WIKI/ADBE - Adj. Close</th>\n",
       "      <th>WIKI/AKAM - Adj. Close</th>\n",
       "      <th>WIKI/ALXN - Adj. Close</th>\n",
       "      <th>WIKI/GOOGL - Adj. Close</th>\n",
       "      <th>WIKI/AMZN - Adj. Close</th>\n",
       "      <th>WIKI/AAL - Adj. Close</th>\n",
       "      <th>WIKI/AMGN - Adj. Close</th>\n",
       "      <th>WIKI/ADI - Adj. Close</th>\n",
       "      <th>WIKI/AAPL - Adj. Close</th>\n",
       "      <th>...</th>\n",
       "      <th>WIKI/FOX - Adj. Close</th>\n",
       "      <th>WIKI/ULTA - Adj. Close</th>\n",
       "      <th>WIKI/VRSK - Adj. Close</th>\n",
       "      <th>WIKI/VRTX - Adj. Close</th>\n",
       "      <th>WIKI/VIAB - Adj. Close</th>\n",
       "      <th>WIKI/VOD - Adj. Close</th>\n",
       "      <th>WIKI/WBA - Adj. Close</th>\n",
       "      <th>WIKI/WDC - Adj. Close</th>\n",
       "      <th>WIKI/WYNN - Adj. Close</th>\n",
       "      <th>WIKI/XLNX - Adj. Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>11.477656</td>\n",
       "      <td>28.57</td>\n",
       "      <td>32.93</td>\n",
       "      <td>70.57</td>\n",
       "      <td>333.735209</td>\n",
       "      <td>179.03</td>\n",
       "      <td>13.303512</td>\n",
       "      <td>56.676913</td>\n",
       "      <td>30.734273</td>\n",
       "      <td>52.848787</td>\n",
       "      <td>...</td>\n",
       "      <td>15.59802</td>\n",
       "      <td>63.384438</td>\n",
       "      <td>39.36</td>\n",
       "      <td>32.23</td>\n",
       "      <td>40.245928</td>\n",
       "      <td>26.898992</td>\n",
       "      <td>29.271975</td>\n",
       "      <td>27.306007</td>\n",
       "      <td>91.264785</td>\n",
       "      <td>28.03994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            WIKI/ATVI - Adj. Close  WIKI/ADBE - Adj. Close  \\\n",
       "Date                                                         \n",
       "2012-01-03               11.477656                   28.57   \n",
       "\n",
       "            WIKI/AKAM - Adj. Close  WIKI/ALXN - Adj. Close  \\\n",
       "Date                                                         \n",
       "2012-01-03                   32.93                   70.57   \n",
       "\n",
       "            WIKI/GOOGL - Adj. Close  WIKI/AMZN - Adj. Close  \\\n",
       "Date                                                          \n",
       "2012-01-03               333.735209                  179.03   \n",
       "\n",
       "            WIKI/AAL - Adj. Close  WIKI/AMGN - Adj. Close  \\\n",
       "Date                                                        \n",
       "2012-01-03              13.303512               56.676913   \n",
       "\n",
       "            WIKI/ADI - Adj. Close  WIKI/AAPL - Adj. Close  \\\n",
       "Date                                                        \n",
       "2012-01-03              30.734273               52.848787   \n",
       "\n",
       "                     ...            WIKI/FOX - Adj. Close  \\\n",
       "Date                 ...                                    \n",
       "2012-01-03           ...                         15.59802   \n",
       "\n",
       "            WIKI/ULTA - Adj. Close  WIKI/VRSK - Adj. Close  \\\n",
       "Date                                                         \n",
       "2012-01-03               63.384438                   39.36   \n",
       "\n",
       "            WIKI/VRTX - Adj. Close  WIKI/VIAB - Adj. Close  \\\n",
       "Date                                                         \n",
       "2012-01-03                   32.23               40.245928   \n",
       "\n",
       "            WIKI/VOD - Adj. Close  WIKI/WBA - Adj. Close  \\\n",
       "Date                                                       \n",
       "2012-01-03              26.898992              29.271975   \n",
       "\n",
       "            WIKI/WDC - Adj. Close  WIKI/WYNN - Adj. Close  \\\n",
       "Date                                                        \n",
       "2012-01-03              27.306007               91.264785   \n",
       "\n",
       "            WIKI/XLNX - Adj. Close  \n",
       "Date                                \n",
       "2012-01-03                28.03994  \n",
       "\n",
       "[1 rows x 88 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- toPredictIndex: First Day of target\n",
    "- PredictionTimepoints: number of datapoints for NN input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1008\n"
     ]
    }
   ],
   "source": [
    "# Number of companies (here: 88)\n",
    "NumberofCompanies = 88\n",
    "#Define starting point for target and number of timepoints that are used as input (org: 250, 250)\n",
    "PredictionTimepoints = 250\n",
    "FirstIndex = PredictionTimepoints\n",
    "\n",
    "MaxPoints = mydata.shape[0]-FirstIndex\n",
    "print(MaxPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data preparation for ANNs:\n",
    "- In the next cell , inputs and targets are created for the NNs (specify with/without prior normalization, MLP/CNN, custom CNN/transfer learning CNN, stock prediction absolute/percent)\n",
    "- Details: For all companies create input vector of past stock prices and output vector of the next day.\n",
    "- Note: Three channel images are requiredfor transfer learning (here:Xception)! Solution: all 3 channels contain\n",
    "the same pixels\n",
    "\n",
    "Output: Complete input and output for multivariate in and output (MLP type 2 and CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP = True# False => CNN\n",
    "normalization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 250, 88)\n",
      "(1008, 88)\n"
     ]
    }
   ],
   "source": [
    "# create np array for Data Collection\n",
    "DataCollection = np.empty([1,PredictionTimepoints,NumberofCompanies])\n",
    "# Create np array for Target Collection\n",
    "TargetCollection = np.empty([1,NumberofCompanies])\n",
    "\n",
    "# Create copy of data frame for handling\n",
    "mydataPP = mydata.copy(deep=True)\n",
    "# Set StartIndex to FirstIndex\n",
    "toPredictIndex = FirstIndex\n",
    "\n",
    "#Normalization if required\n",
    "if (normalization==True):\n",
    "    mydataNP = mydata.values\n",
    "    scaler = MinMaxScaler()  \n",
    "    mydataNormalizedNP = scaler.fit_transform(mydataNP)\n",
    "    mydataPP = pd.DataFrame(mydataNormalizedNP)\n",
    "\n",
    "#FIRST define target day, THEN extract image of past data\n",
    "\n",
    "for i in range(MaxPoints):\n",
    "    \n",
    "    #START CREATE OUTPUT VECTORS\n",
    "    \n",
    "    PredictTemp = mydataPP.iloc[toPredictIndex]\n",
    "   \n",
    "    arrayPredtemp = np.array(PredictTemp, np.float32)[newaxis,:]\n",
    "    TargetCollection = np.append(TargetCollection, arrayPredtemp, axis=0)    \n",
    "    \n",
    "    \n",
    "    #START CREATE INPUT VECTORS (IMAGES)\n",
    "    end = toPredictIndex # e.g. first = 250\n",
    "    start = end  - PredictionTimepoints # e.g. first = 0\n",
    "     \n",
    "    AdjCloseTemp = mydataPP.iloc[start : end] # e.g. 0 - 249 inclusive, as last index is not sliced\n",
    "    \n",
    "#     #Ordering for CNN\n",
    "#     if (MLP==False):\n",
    "#         arrayTemp = np.flip(np.transpose(np.array(AdjCloseTemp, np.float32)[:, :]),axis=1)\n",
    "#         AdjCloseTemp = pd.DataFrame(arrayTemp)\n",
    "#         AdjCloseTemp[\"cluster\"] = preds\n",
    "#         AdjCloseTemp = AdjCloseTemp.sort_values(\"cluster\")\n",
    "#         AdjCloseTemp = AdjCloseTemp.drop('cluster', 1)\n",
    "#         arrayTemp = np.transpose(np.flip(AdjCloseTemp.values,axis=1))\n",
    "#         AdjCloseTemp = pd.DataFrame(arrayTemp)\n",
    "        \n",
    "    AdjCloseTemp_Array = AdjCloseTemp.values\n",
    "    \n",
    "\n",
    "    arrayAdjClosedTemp = np.array(AdjCloseTemp_Array, np.float32)[newaxis, :,:]\n",
    "    DataCollection = np.append(DataCollection, arrayAdjClosedTemp, axis=0)\n",
    "    \n",
    "    #END CREATE IMAGES\n",
    "    \n",
    "    toPredictIndex += 1\n",
    "\n",
    "   \n",
    "DataCollection = DataCollection[1:DataCollection.shape[0],:,:]\n",
    "TargetCollection = TargetCollection[1:TargetCollection.shape[0],:]\n",
    "\n",
    "#For CNN reshape is required\n",
    "if (MLP==False):\n",
    "    DataCollection  = DataCollection.reshape(DataCollection.shape[0], DataCollection.shape[1], DataCollection.shape[2], 1)\n",
    "    \n",
    "print(DataCollection.shape)\n",
    "print(TargetCollection.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MLP type 2 Data Splitting:__Train, test , validation split for MLP type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of Training samples (70 %), Validation (15%) and Testsamples (15%)\n",
    "TrainingSamples = int(MaxPoints * 0.7)\n",
    "ValidationSamples = int((MaxPoints-TrainingSamples)/2)\n",
    "TestSamples = MaxPoints - TrainingSamples - ValidationSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(705, 250, 88)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.copy(DataCollection[:TrainingSamples,:])\n",
    "y_train = np.copy(TargetCollection[:TrainingSamples,:])\n",
    "X_valid = np.copy(DataCollection[TrainingSamples-1:TrainingSamples+ValidationSamples,:])\n",
    "y_valid = np.copy(TargetCollection[TrainingSamples-1:TrainingSamples+ValidationSamples,:])\n",
    "X_test = np.copy(DataCollection[TrainingSamples+ValidationSamples-1:,:])\n",
    "y_test = np.copy(TargetCollection[TrainingSamples+ValidationSamples-1:,:])\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Network Training and scoring (Keras models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP type 2:for multi company in and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP_B2_LSTM():\n",
    "    model = Sequential()\n",
    "  #  model.add(Flatten(input_shape=(PredictionTimepoints, NumberofCompanies)))\n",
    "\n",
    "    model.add(LSTM(50, input_shape=(PredictionTimepoints, NumberofCompanies),  activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "    model.add(Dense(88))\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"adamax\", metrics=['mse'])\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 705 samples, validate on 152 samples\n",
      "Epoch 1/2000\n",
      "705/705 [==============================] - 32s 45ms/step - loss: 267709554987368448.0000 - mean_squared_error: 267709554987368448.0000 - val_loss: 206002393147285564293120.0000 - val_mean_squared_error: 206002393147285564293120.0000\n",
      "Epoch 2/2000\n",
      "705/705 [==============================] - 22s 32ms/step - loss: 6630399781598262198272.0000 - mean_squared_error: 6630399781598262198272.0000 - val_loss: 4336287640175206293133328384.0000 - val_mean_squared_error: 4336287640175206293133328384.0000\n",
      "Epoch 3/2000\n",
      "705/705 [==============================] - 23s 33ms/step - loss: 48672790314580871810646016.0000 - mean_squared_error: 48672790314580871810646016.0000 - val_loss: 558683171076747154161664.0000 - val_mean_squared_error: 558683171076747154161664.0000\n",
      "Epoch 4/2000\n",
      "705/705 [==============================] - 24s 34ms/step - loss: 6220546882759643103232.0000 - mean_squared_error: 6220546882759643103232.0000 - val_loss: 458570308580199082906972323840.0000 - val_mean_squared_error: 458570308580199082906972323840.0000\n",
      "Epoch 5/2000\n",
      "705/705 [==============================] - 24s 34ms/step - loss: 730250667350129661116940288.0000 - mean_squared_error: 730250667350129661116940288.0000 - val_loss: 7128099598226516040418228952891392.0000 - val_mean_squared_error: 7128099598226516040418228952891392.0000\n",
      "Epoch 6/2000\n",
      "705/705 [==============================] - 24s 34ms/step - loss: 835382638828930537095168.0000 - mean_squared_error: 835382638828930537095168.0000 - val_loss: 2407181988773162567495530810703872.0000 - val_mean_squared_error: 2407181988773162567495530810703872.0000\n",
      "Epoch 7/2000\n",
      "705/705 [==============================] - 24s 35ms/step - loss: 54509048954880.0000 - mean_squared_error: 54509048954880.0000 - val_loss: 658505646449832595271412355170304.0000 - val_mean_squared_error: 658505646449832595271412355170304.0000\n",
      "Epoch 8/2000\n",
      "705/705 [==============================] - 24s 35ms/step - loss: 2434788841077786755978632364032.0000 - mean_squared_error: 2434788841077786755978632364032.0000 - val_loss: 169131372129483216808544978337792.0000 - val_mean_squared_error: 169131372129483216808544978337792.0000\n",
      "Epoch 9/2000\n",
      "705/705 [==============================] - 24s 35ms/step - loss: 894009257201483028928086933504.0000 - mean_squared_error: 894009257201483028928086933504.0000 - val_loss: 22570021246482205543110157533184.0000 - val_mean_squared_error: 22570021246482205543110157533184.0000\n",
      "Epoch 10/2000\n",
      "705/705 [==============================] - 24s 35ms/step - loss: 142725243913924073226256777216.0000 - mean_squared_error: 142725243913924073226256777216.0000 - val_loss: 2234949322970847354026236837888.0000 - val_mean_squared_error: 2234949322970847354026236837888.0000\n",
      "Epoch 11/2000\n",
      "705/705 [==============================] - 25s 36ms/step - loss: 38018320425890031199563284480.0000 - mean_squared_error: 38018320425890031199563284480.0000 - val_loss: 370754654843097815842388180992.0000 - val_mean_squared_error: 370754654843097815842388180992.0000\n",
      "Epoch 12/2000\n",
      "705/705 [==============================] - 25s 35ms/step - loss: 8563316248219499039779979264.0000 - mean_squared_error: 8563316248219499039779979264.0000 - val_loss: 61483813786483851267338665984.0000 - val_mean_squared_error: 61483813786483851267338665984.0000\n",
      "Epoch 13/2000\n",
      "705/705 [==============================] - 25s 35ms/step - loss: 1519995330964929599076564992.0000 - mean_squared_error: 1519995330964929599076564992.0000 - val_loss: 9831343664958862541746864128.0000 - val_mean_squared_error: 9831343664958862541746864128.0000\n",
      "Epoch 14/2000\n",
      "705/705 [==============================] - 25s 36ms/step - loss: 226872354845137190676070400.0000 - mean_squared_error: 226872354845137190676070400.0000 - val_loss: 1583397823363937455715647488.0000 - val_mean_squared_error: 1583397823363937455715647488.0000\n",
      "Epoch 15/2000\n",
      "705/705 [==============================] - 25s 36ms/step - loss: 16133854408259700615282688.0000 - mean_squared_error: 16133854408259700615282688.0000 - val_loss: 218886722442140178362400768.0000 - val_mean_squared_error: 218886722442140178362400768.0000\n",
      "Epoch 16/2000\n",
      "705/705 [==============================] - 26s 37ms/step - loss: 2806635217326760859271168.0000 - mean_squared_error: 2806635217326760859271168.0000 - val_loss: 37085537199970493373874176.0000 - val_mean_squared_error: 37085537199970493373874176.0000\n",
      "Epoch 17/2000\n",
      "705/705 [==============================] - 26s 37ms/step - loss: 569703083048155549270016.0000 - mean_squared_error: 569703083048155549270016.0000 - val_loss: 5114796175225466877116416.0000 - val_mean_squared_error: 5114796175225466877116416.0000\n",
      "Epoch 18/2000\n",
      "705/705 [==============================] - 26s 37ms/step - loss: 50792087489843837272064.0000 - mean_squared_error: 50792087489843837272064.0000 - val_loss: 680135561280531329974272.0000 - val_mean_squared_error: 680135561280531329974272.0000\n",
      "Epoch 19/2000\n",
      "705/705 [==============================] - 26s 37ms/step - loss: 6223549094861238960128.0000 - mean_squared_error: 6223549094861238960128.0000 - val_loss: 156457482998649811107840.0000 - val_mean_squared_error: 156457482998649811107840.0000\n",
      "Epoch 20/2000\n",
      "705/705 [==============================] - 26s 37ms/step - loss: 295090765759080562688.0000 - mean_squared_error: 295090765759080562688.0000 - val_loss: 47627642218871212998656.0000 - val_mean_squared_error: 47627642218871212998656.0000\n",
      "Epoch 21/2000\n",
      "705/705 [==============================] - 26s 37ms/step - loss: 22701170176970194944.0000 - mean_squared_error: 22701170176970194944.0000 - val_loss: 7720342386066006736896.0000 - val_mean_squared_error: 7720342386066006736896.0000\n",
      "Epoch 22/2000\n",
      "705/705 [==============================] - 26s 37ms/step - loss: 3769770723585818624.0000 - mean_squared_error: 3769770723585818624.0000 - val_loss: 3808028516547307765760.0000 - val_mean_squared_error: 3808028516547307765760.0000\n",
      "Epoch 23/2000\n",
      "705/705 [==============================] - 26s 38ms/step - loss: 295454442424107008.0000 - mean_squared_error: 295454442424107008.0000 - val_loss: 1122108579643819819008.0000 - val_mean_squared_error: 1122108579643819819008.0000\n",
      "Epoch 24/2000\n",
      "705/705 [==============================] - 27s 38ms/step - loss: 132067959429398528.0000 - mean_squared_error: 132067959429398528.0000 - val_loss: 252748344275008225280.0000 - val_mean_squared_error: 252748344275008225280.0000\n",
      "Epoch 25/2000\n",
      "705/705 [==============================] - 27s 38ms/step - loss: 29490727218053120.0000 - mean_squared_error: 29490727218053120.0000 - val_loss: 138779856280584454144.0000 - val_mean_squared_error: 138779856280584454144.0000\n",
      "Epoch 26/2000\n",
      "705/705 [==============================] - 27s 38ms/step - loss: 5033652278788096.0000 - mean_squared_error: 5033652278788096.0000 - val_loss: 32068633212645015552.0000 - val_mean_squared_error: 32068633212645015552.0000\n",
      "Epoch 27/2000\n",
      "705/705 [==============================] - 27s 38ms/step - loss: 1609937889263616.0000 - mean_squared_error: 1609937889263616.0000 - val_loss: 6803560249375588352.0000 - val_mean_squared_error: 6803560249375588352.0000\n",
      "Epoch 28/2000\n",
      "705/705 [==============================] - 27s 38ms/step - loss: 302460220473344.0000 - mean_squared_error: 302460220473344.0000 - val_loss: 3927693853561192448.0000 - val_mean_squared_error: 3927693853561192448.0000\n",
      "Epoch 29/2000\n",
      "705/705 [==============================] - 27s 38ms/step - loss: 106129262641152.0000 - mean_squared_error: 106129262641152.0000 - val_loss: 1019705362980274176.0000 - val_mean_squared_error: 1019705362980274176.0000\n",
      "Epoch 30/2000\n",
      "705/705 [==============================] - 27s 39ms/step - loss: 36406357393408.0000 - mean_squared_error: 36406357393408.0000 - val_loss: 476120936060289024.0000 - val_mean_squared_error: 476120936060289024.0000\n",
      "Epoch 31/2000\n",
      "705/705 [==============================] - 27s 38ms/step - loss: 22549012414464.0000 - mean_squared_error: 22549012414464.0000 - val_loss: 119730605742096384.0000 - val_mean_squared_error: 119730605742096384.0000\n",
      "Epoch 32/2000\n",
      "705/705 [==============================] - 27s 39ms/step - loss: 13030120226816.0000 - mean_squared_error: 13030120226816.0000 - val_loss: 49852286700093440.0000 - val_mean_squared_error: 49852286700093440.0000\n",
      "Epoch 33/2000\n",
      "705/705 [==============================] - 28s 39ms/step - loss: 9710847655936.0000 - mean_squared_error: 9710847655936.0000 - val_loss: 26508819471269888.0000 - val_mean_squared_error: 26508819471269888.0000\n",
      "Epoch 34/2000\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# specify ANN\n",
    "\n",
    "estimator = KerasRegressor(build_fn=MLP_B2_LSTM, epochs=epochs, batch_size=TrainingSamples, verbose=1)\n",
    "\n",
    "# Enter checkpoint filename here\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch_MLPtype2_B2LSTM_Timepoints'+str(PredictionTimepoints)+'.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "estimator.fit(X_train, y_train,  validation_data=(X_valid, y_valid),callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model = MLP_B2_LSTM()\n",
    "model.load_weights('saved_models/weights.best.from_scratch_MLPtype2_B2LSTM_Timepoints'+str(PredictionTimepoints)+'.hdf5')\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate the Trained networks => Predict Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Profit for multi company forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale targets and transform them into percentual changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cont = pd.DataFrame(scaler.inverse_transform(TargetCollection),copy=True)\n",
    "y_temp = y_test_cont.copy(deep=True)\n",
    "i=0\n",
    "\n",
    "for entry in TargetCollection:\n",
    "\n",
    "    if (i!=0):\n",
    "        #print(y_test_cont.iloc[i])\n",
    "        y_test_cont.iloc[i] = 100*(y_temp.iloc[i]-y_temp.iloc[i-1])/y_temp.iloc[i-1]\n",
    "    i=i+1\n",
    "\n",
    "y_test_cont=y_test_cont[1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict for y_pred_cont and transform y_pred_cont to percentual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MLP_B2_LSTM()\n",
    "model.load_weights('saved_models/weights.best.from_scratch_MLPtype2_B2LSTM_Timepoints'+str(PredictionTimepoints)+'.hdf5')\n",
    "#X_test_cont = np.copy(DataCollection[1:,:,:,:]) # For CNN\n",
    "X_test_cont = np.copy(DataCollection[1:,:,:]) #for MLP\n",
    "y_pred_cont = model.predict(X_test_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_cont_df = pd.DataFrame(scaler.inverse_transform(y_pred_cont),copy=True)\n",
    "i=0\n",
    "for entry in y_pred_cont:\n",
    "    \n",
    "    #print(y_test_cont.iloc[i])\n",
    "    y_pred_cont_df.iloc[i] = 100*(y_pred_cont_df.iloc[i]-y_temp.iloc[i])/y_temp.iloc[i]\n",
    "    i=i+1\n",
    "y_pred_cont=y_pred_cont_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check predictions (pred) and actual values (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_pred_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_test_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the maximum value in each timestep = 1 , other = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_cont_norm = (y_pred_cont == y_pred_cont.max(axis=1)[:,None]).astype(int)\n",
    "\n",
    "y_pred_cont_norm_df = pd.DataFrame(y_pred_cont_norm,copy=True)\n",
    "y_test_cont_df = pd.DataFrame(y_test_cont,copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cont_norm.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate profit: Every day only the maximum predicted positive stock change is used to invest all of startmoney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only profit from test range on\n",
    "y_test_cont_df = y_test_cont_df[862:]\n",
    "y_pred_cont_norm_df = y_pred_cont_norm_df[862:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "profitMulti = []\n",
    "MaxPercentage =[]\n",
    "RandomInvest = []\n",
    "RandomPercentage = []\n",
    "startmoney = 1.0\n",
    "startmoneyrandom = 1.0\n",
    "\n",
    "for row in y_test_cont_df.index:\n",
    "    for column in y_test_cont_df.columns:\n",
    "    \n",
    "        if y_pred_cont_norm_df[column][row] != 0:\n",
    "            startmoney=startmoney+(startmoney*y_test_cont_df[column][row]*y_pred_cont_norm_df[column][row]/100)\n",
    "            profitMulti.append(startmoney)\n",
    "            MaxPercentage.append(y_test_cont_df[column][row]*y_pred_cont_norm_df[column][row])\n",
    "    startmoneyrandom = startmoneyrandom+(startmoneyrandom*y_test_cont_df[randint(0,len(y_test_cont_df.columns)-1)][row]/100)\n",
    "    RandomInvest.append(startmoneyrandom) \n",
    "    RandomPercentage.append(y_test_cont_df[randint(0,len(y_test_cont_df.columns)-1)][row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Profit over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(profitMulti, label=\"Algorithm\")\n",
    "plt.plot(RandomInvest,label=\"Random\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Money\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profitMulti[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(MaxPercentage)\n",
    "plt.plot(RandomPercentage)\n",
    "plt.ylabel(\"percentual change used for profit\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.describe(MaxPercentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.describe(RandomPercentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(MaxPercentage, bins=50,alpha=0.5)\n",
    "plt.hist(RandomPercentage, bins=50,alpha=0.5)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"percentual change used for profit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
